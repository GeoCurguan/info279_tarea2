{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# Establecer una variable de entorno\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "\n",
    "print(os.getenv('OPENAI_API_KEY'))\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"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\"\n",
    "MODEL=\"HuggingFaceH4/zephyr-7b-alpha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/\"+MODEL\n",
    "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No se si sirve en csv o si no hay que pasarlo a txt antes\n",
    "df = pd.read_csv('noticias-losrios-2023.csv')\n",
    "df2 = pd.read_csv('test/dataset-test-desafio2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage, ServiceContext\n",
    "from llama_index.llms import HuggingFaceInferenceAPI, HuggingFaceLLM\n",
    "\n",
    "# # Solo una vez guardar vectores en storage\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "# index = VectorStoreIndex.from_documents(documents)\n",
    "# index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remotely_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diegoh/miniconda3/envs/tal/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "remotely_run = HuggingFaceInferenceAPI(\n",
    "    model_name=\"HuggingFaceH4/zephyr-7b-alpha\", token=HF_TOKEN\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(chunk_size=512, llm=remotely_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_en = \"\"\"Read the following news article and provide information on the main event and its specific location. \n",
    "Your response should be formatted in CSV with one columns: \"main_event\".\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_es = \"\"\"Lee el siguiente artículo de noticias y proporciona información sobre el evento principal y la ubicación específica del evento principal.\n",
    "Tu respuesta debe tener formato JSON con dos campos: \"main_event\" y \"location\".\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-es-en\"\n",
    "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "\n",
    "def query_translate(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2892\n",
      "What started as the dream of two educators who went out to run through the city’s forests, today is a reality. Carola Püschel and Claudia Contreras wondered, how to protect these places so that more people could enjoy them, and their response was children. “As long as they know them and live them, there is no doubt that they will take care of them,” Claudia adds. It was so two years ago, in the middle of a pandemic, and with the urgent need to get the children out of their homes, the opening of theClub del Bosque Valdiviano, kindergarten and workshops. “We were surprised by the high demand for tuition, we realized that there was a community that needed a project like ours,” adds Carola. The project is based on outdoor learning, a philosophy proven years ago in countries such as England, Denmark and Sweden. Precisely based on these experiences, with training abroad and at the national level, the Club del Bosque has been building its own method. “We took what we didIt is best suited to our reality of other experiences and we are building a way of educating, but under standards that have been tested and systematized in other countries,” explained the leaders of this new project. In order to expand their model to more children in Valdivia and “Ojala de la region” decide in 2022 to create the Forest Club Foundation, through which they are working with the Rural School of Punucapa, they are part of the Valdivia Sustainable Consortium and they are carrying outThe first major research on open-air education in Chile, by the University of Development, among other things. In parallel to the creation of the Foundation, the parent community began to push the idea of making a college, an idea that after much effort has come to fruition and in March 2024 the Forest College opens its doors. Its curriculum framework is based on outdoor learning and has as its center the integral development of children. “Whenever we make a decision we make a decision on the subject, we will have to“We have a daily day from 08:30 to 16:30 based on the outdoor learning methodology, but with all the contents of mathematics, language, science and much more,” adds Carola. “The school is also a way to give continuity to the garden’s educational project and the pl“It will be the meeting place for our entire community, after all, it was they who encouraged us to want to change education from Valdivia,” they add. For more information https://foundacionclubdelbosque.cl/\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#Responder pregunta sobre el evento principal\n",
    "new = df2['text'][21]\n",
    "\n",
    "size_of_new = len(new)\n",
    "iter = math.ceil(size_of_new / 512)\n",
    "translated = ''\n",
    "print(iter, size_of_new)\n",
    "for i in range(0, iter):\n",
    "    start_index = i * 512\n",
    "    end_index = (i + 1) * 512\n",
    "    fragment = new[start_index:end_index]\n",
    "\n",
    "    output = query_translate({\n",
    "        \"inputs\": f'{fragment}',\n",
    "    })\n",
    "    # Asumiendo que el resultado de query_translate es una lista de diccionarios\n",
    "    translated += str(output[0]['translation_text'])\n",
    "\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_event = query_engine.query(question_en+\" \"+translated).response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The main event is the opening of the Forest College, which is a new college based on outdoor learning and integral development of children. The location is not explicitly mentioned in the article, but it is mentioned that the Forest Club Foundation is working with the Rural School of Punucapa and is part of the Valdivia Sustainable Consortium. It is also mentioned that the Forest College will be the meeting place for the entire community, but no specific location is given.\n"
     ]
    }
   ],
   "source": [
    "print(main_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFRobertaForMaskedLM\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = TFRobertaForMaskedLM.from_pretrained(\"roberta-base\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is <mask>.\", return_tensors=\"tf\")\n",
    "logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of <mask>\n",
    "mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])\n",
    "selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
    "\n",
    "predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n",
    "tokenizer.decode(predicted_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": " (Request ID: YQhSWbNVHGHPbiUFS11IC)\n\nBad request:\nError in `inputs`: value is not a valid dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    262\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/deepset/roberta-base-squad2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/diegoh/universidad/tal/tarea_2/info279_tarea2/main.ipynb Celda 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diegoh/universidad/tal/tarea_2/info279_tarea2/main.ipynb#X61sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m index_roberta \u001b[39m=\u001b[39m VectorStoreIndex\u001b[39m.\u001b[39mfrom_documents(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diegoh/universidad/tal/tarea_2/info279_tarea2/main.ipynb#X61sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     documents, service_context\u001b[39m=\u001b[39mservice_context\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diegoh/universidad/tal/tarea_2/info279_tarea2/main.ipynb#X61sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diegoh/universidad/tal/tarea_2/info279_tarea2/main.ipynb#X61sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m query_engine \u001b[39m=\u001b[39m index_roberta\u001b[39m.\u001b[39mas_query_engine()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/diegoh/universidad/tal/tarea_2/info279_tarea2/main.ipynb#X61sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m address \u001b[39m=\u001b[39m query_engine\u001b[39m.\u001b[39;49mquery(\u001b[39m\"\u001b[39;49m\u001b[39mwhat is the address of the main event?\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49mmain_event)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diegoh/universidad/tal/tarea_2/info279_tarea2/main.ipynb#X61sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(address)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diegoh/universidad/tal/tarea_2/info279_tarea2/main.ipynb#X61sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# output_roberta = query_roberta({\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diegoh/universidad/tal/tarea_2/info279_tarea2/main.ipynb#X61sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# \t\"inputs\": {\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diegoh/universidad/tal/tarea_2/info279_tarea2/main.ipynb#X61sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# \t\t\"question\": \"what is the address of the main event?\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diegoh/universidad/tal/tarea_2/info279_tarea2/main.ipynb#X61sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# })\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diegoh/universidad/tal/tarea_2/info279_tarea2/main.ipynb#X61sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# print(output_roberta['answer'])\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/llama_index/indices/query/base.py:32\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     31\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 32\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/llama_index/query_engine/retriever_query_engine.py:182\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    176\u001b[0m         nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretrieve(query_bundle)\n\u001b[1;32m    178\u001b[0m         retrieve_event\u001b[39m.\u001b[39mon_end(\n\u001b[1;32m    179\u001b[0m             payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mNODES: nodes},\n\u001b[1;32m    180\u001b[0m         )\n\u001b[0;32m--> 182\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_response_synthesizer\u001b[39m.\u001b[39;49msynthesize(\n\u001b[1;32m    183\u001b[0m         query\u001b[39m=\u001b[39;49mquery_bundle,\n\u001b[1;32m    184\u001b[0m         nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m    185\u001b[0m     )\n\u001b[1;32m    187\u001b[0m     query_event\u001b[39m.\u001b[39mon_end(payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mRESPONSE: response})\n\u001b[1;32m    189\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/llama_index/response_synthesizers/base.py:148\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     query \u001b[39m=\u001b[39m QueryBundle(query_str\u001b[39m=\u001b[39mquery)\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    146\u001b[0m     CBEventType\u001b[39m.\u001b[39mSYNTHESIZE, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    147\u001b[0m ) \u001b[39mas\u001b[39;00m event:\n\u001b[0;32m--> 148\u001b[0m     response_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m    149\u001b[0m         query_str\u001b[39m=\u001b[39;49mquery\u001b[39m.\u001b[39;49mquery_str,\n\u001b[1;32m    150\u001b[0m         text_chunks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    151\u001b[0m             n\u001b[39m.\u001b[39;49mnode\u001b[39m.\u001b[39;49mget_content(metadata_mode\u001b[39m=\u001b[39;49mMetadataMode\u001b[39m.\u001b[39;49mLLM) \u001b[39mfor\u001b[39;49;00m n \u001b[39min\u001b[39;49;00m nodes\n\u001b[1;32m    152\u001b[0m         ],\n\u001b[1;32m    153\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    156\u001b[0m     additional_source_nodes \u001b[39m=\u001b[39m additional_source_nodes \u001b[39mor\u001b[39;00m []\n\u001b[1;32m    157\u001b[0m     source_nodes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(nodes) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/llama_index/response_synthesizers/compact_and_refine.py:38\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m new_texts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m     39\u001b[0m     query_str\u001b[39m=\u001b[39;49mquery_str,\n\u001b[1;32m     40\u001b[0m     text_chunks\u001b[39m=\u001b[39;49mnew_texts,\n\u001b[1;32m     41\u001b[0m     prev_response\u001b[39m=\u001b[39;49mprev_response,\n\u001b[1;32m     42\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m     43\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/llama_index/response_synthesizers/refine.py:127\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mfor\u001b[39;00m text_chunk \u001b[39min\u001b[39;00m text_chunks:\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m prev_response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[39m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         \u001b[39m# is an answer, then return it\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_give_response_single(\n\u001b[1;32m    128\u001b[0m             query_str, text_chunk, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs\n\u001b[1;32m    129\u001b[0m         )\n\u001b[1;32m    130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[39m# refine response if possible\u001b[39;00m\n\u001b[1;32m    132\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_refine_response_single(\n\u001b[1;32m    133\u001b[0m             prev_response, query_str, text_chunk, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kwargs\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/llama_index/response_synthesizers/refine.py:182\u001b[0m, in \u001b[0;36mRefine._give_response_single\u001b[0;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streaming:\n\u001b[1;32m    179\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m         structured_response \u001b[39m=\u001b[39m cast(\n\u001b[1;32m    181\u001b[0m             StructuredRefineResponse,\n\u001b[0;32m--> 182\u001b[0m             program(\n\u001b[1;32m    183\u001b[0m                 context_str\u001b[39m=\u001b[39;49mcur_text_chunk,\n\u001b[1;32m    184\u001b[0m                 output_cls\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_output_cls,\n\u001b[1;32m    185\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m    186\u001b[0m             ),\n\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m         query_satisfied \u001b[39m=\u001b[39m structured_response\u001b[39m.\u001b[39mquery_satisfied\n\u001b[1;32m    189\u001b[0m         \u001b[39mif\u001b[39;00m query_satisfied:\n",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/llama_index/response_synthesizers/refine.py:53\u001b[0m, in \u001b[0;36mDefaultRefineProgram.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m StructuredRefineResponse:\n\u001b[0;32m---> 53\u001b[0m     answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm_predictor\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m     54\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prompt,\n\u001b[1;32m     55\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m StructuredRefineResponse(answer\u001b[39m=\u001b[39manswer, query_satisfied\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/llama_index/llm_predictor/base.py:181\u001b[0m, in \u001b[0;36mLLMPredictor.predict\u001b[0;34m(self, prompt, output_cls, **prompt_args)\u001b[0m\n\u001b[1;32m    179\u001b[0m     formatted_prompt \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mformat(llm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprompt_args)\n\u001b[1;32m    180\u001b[0m     formatted_prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_prompt(formatted_prompt)\n\u001b[0;32m--> 181\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm\u001b[39m.\u001b[39;49mcomplete(formatted_prompt)\n\u001b[1;32m    182\u001b[0m     output \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mtext\n\u001b[1;32m    184\u001b[0m logger\u001b[39m.\u001b[39mdebug(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/llama_index/llms/huggingface.py:511\u001b[0m, in \u001b[0;36mHuggingFaceInferenceAPI.complete\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcomplete\u001b[39m(\u001b[39mself\u001b[39m, prompt: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CompletionResponse:\n\u001b[1;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m CompletionResponse(\n\u001b[0;32m--> 511\u001b[0m         text\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sync_client\u001b[39m.\u001b[39;49mtext_generation(\n\u001b[1;32m    512\u001b[0m             prompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mmax_new_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_output}, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs}\n\u001b[1;32m    513\u001b[0m         )\n\u001b[1;32m    514\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:1527\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         _set_as_non_tgi(model)\n\u001b[1;32m   1507\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_generation(  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m             prompt\u001b[39m=\u001b[39mprompt,\n\u001b[1;32m   1509\u001b[0m             details\u001b[39m=\u001b[39mdetails,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             decoder_input_details\u001b[39m=\u001b[39mdecoder_input_details,\n\u001b[1;32m   1526\u001b[0m         )\n\u001b[0;32m-> 1527\u001b[0m     raise_text_generation_error(e)\n\u001b[1;32m   1529\u001b[0m \u001b[39m# Parse output\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39mif\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/huggingface_hub/inference/_text_generation.py:480\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[0;34m(http_error)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[39mraise\u001b[39;00m ValidationError(message) \u001b[39mfrom\u001b[39;00m \u001b[39mhttp_error\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[39m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m \u001b[39mraise\u001b[39;00m http_error\n",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:1503\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[39m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1503\u001b[0m     bytes_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpost(json\u001b[39m=\u001b[39;49mpayload, model\u001b[39m=\u001b[39;49mmodel, task\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m, stream\u001b[39m=\u001b[39;49mstream)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1505\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, BadRequestError) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:238\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m InferenceTimeoutError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInference call timed out: \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merror\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     hf_raise_for_status(response)\n\u001b[1;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39miter_lines() \u001b[39mif\u001b[39;00m stream \u001b[39melse\u001b[39;00m response\u001b[39m.\u001b[39mcontent\n\u001b[1;32m    240\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/anaconda3/envs/tal/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:299\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n\u001b[1;32m    296\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    297\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mBad request for \u001b[39m\u001b[39m{\u001b[39;00mendpoint_name\u001b[39m}\u001b[39;00m\u001b[39m endpoint:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m endpoint_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mBad request:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     )\n\u001b[0;32m--> 299\u001b[0m     \u001b[39mraise\u001b[39;00m BadRequestError(message, response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[39m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39mraise\u001b[39;00m HfHubHTTPError(\u001b[39mstr\u001b[39m(e), response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m:  (Request ID: YQhSWbNVHGHPbiUFS11IC)\n\nBad request:\nError in `inputs`: value is not a valid dict"
     ]
    }
   ],
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/deepset/roberta-base-squad2\"\n",
    "\n",
    "def query_roberta(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "remotely_run_roberta = HuggingFaceInferenceAPI(\n",
    "    model_name=\"deepset/roberta-base-squad2\", token=HF_TOKEN\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size=512, llm=remotely_run_roberta)\n",
    "\n",
    "index_roberta = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")\n",
    "\n",
    "query_engine = index_roberta.as_query_engine()\n",
    "\n",
    "address = query_engine.query(\"what is the address of the main event?\"+\" \"+main_event)\n",
    "print(address)\n",
    "# output_roberta = query_roberta({\n",
    "# \t\"inputs\": {\n",
    "# \t\t\"question\": \"what is the address of the main event?\",\n",
    "# \t\t\"context\": translated\n",
    "# \t\t},\n",
    "# })\n",
    "# print(output_roberta['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LlamaIndex con HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms import HuggingFaceInferenceAPI, HuggingFaceLLM\n",
    "# from llama_index import StorageContext, load_index_from_storage, ServiceContext\n",
    "\n",
    "# remotely_run = HuggingFaceInferenceAPI(\n",
    "#     model_name=\"HuggingFaceH4/zephyr-7b-alpha\", token=HF_TOKEN\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# service_context = ServiceContext.from_defaults(chunk_size=512, llm=remotely_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, service_context=service_context\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question=\"\"\"Read the following news article and tell me what is the main event and where does is it take place (try to be the more specific). \n",
    "# Your response will be formatted in CSV with two columns: main event, location\"\"\"\n",
    "\n",
    "# #Responder pregunta sobre el evento principal\n",
    "# new = df2['text'][101]\n",
    "# query_engine = index.as_query_engine()\n",
    "# llama_response = query_engine.query(question+\" \"+new)\n",
    "# print(llama_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geopy.geocoders import Nominatim\n",
    "\n",
    "# geolocator = Nominatim(user_agent=\"Sophia\")\n",
    "\n",
    "# location = geolocator.geocode(\"Antofagasta 755, Las Animas, Chile\")\n",
    "\n",
    "# print(location.address)\n",
    "# print((location.latitude, location.longitude))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
